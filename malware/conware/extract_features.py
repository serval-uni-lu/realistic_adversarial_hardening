import os
import subprocess
import math
import shutil
import pefile
import array
import itertools
import traceback
import pandas as pd
import capstone
import numpy as np
import multiprocessing
from collections import Counter
import pickle
import sys
sys.path.append('../../richheader')
from conware.richheader import rich_standalone as rich
#from richheader import rich_standalone as rich
#import util
from conware import util

# borrowed from https://github.com/Te-k/malware-classification/blob/master/generatedata.py
def get_entropy(data):
    if len(data) == 0:
        return 0.0
    occurences = array.array('L', [0]*256)
    for x in data:
        occurences[x if isinstance(x, int) else ord(x)] += 1

    entropy = 0
    for x in occurences:
        if x:
            p_x = float(x) / len(data)
            entropy -= p_x*math.log(p_x, 2)

    return entropy

def byte_histogram(path):
    with open(path, 'rb') as f:
        byteArr = f.read()
        fileSize = len(byteArr)
    freqs = Counter()
    for byte in byteArr:
        freqs[byte] += 1
    freqList = [float(freqs[byte]) / float(fileSize) for byte in range(256)]
    return fileSize, freqList

# borrowed from (then modified a bit) https://kennethghartman.com/calculate-file-entropy/
def file_entropy(path):

    # calculate the frequency of each byte value in the file
    fileSize, freqList = byte_histogram(path)

    # Shannon entropy
    ent = 0.0
    for freq in freqList:
        if freq > 0:
            ent = ent + freq * math.log(freq, 2)
    ent = -ent

    return ent, fileSize


# borrowed (and then a bit modified) from https://github.com/Te-k/malware-classification/blob/master/generatedata.py
def parse_resources(pe):
    entropies = []
    sizes = []
    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
        try:
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                if hasattr(resource_type, 'directory'):
                    for resource_id in resource_type.directory.entries:
                        if hasattr(resource_id, 'directory'):
                            for resource_lang in resource_id.directory.entries:
                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)
                                size = resource_lang.data.struct.Size
                                entropy = get_entropy(data)

                                entropies.append(entropy)
                                sizes.append(size)
        except Exception as e:
            print(e)
            traceback.print_exc()

    assert len(entropies) == len(sizes)
    if len(sizes):
        mean_entropy = sum(entropies) / float(len(entropies))
        min_entropy = min(entropies)
        max_entropy = max(entropies)
        mean_size = sum(sizes) / float(len(sizes))
        min_size = min(sizes)
        max_size = max(sizes)
        resources_nb = len(entropies)
    else:
        mean_entropy    = 0
        min_entropy     = 0
        max_entropy     = 0
        mean_size       = 0
        min_size        = 0
        max_size        = 0
        resources_nb    = 0


    secs = {}

    secs['pesectionProcessed_resourcesMeanEntropy'] = mean_entropy
    secs['pesectionProcessed_resourcesMinEntropy']  = min_entropy
    secs['pesectionProcessed_resourcesMaxEntropy']  = max_entropy

    secs['pesectionProcessed_resourcesMeanSize'] = mean_size
    secs['pesectionProcessed_resourcesMinSize']  = min_size
    secs['pesectionProcessed_resourcesMaxSize']  = max_size

    secs['pesectionProcessed_resources_nb'] = resources_nb

    return secs

def parse_imports(pe):
    dlls = []
    imps = []

    if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
        for entry in pe.DIRECTORY_ENTRY_IMPORT:
            dll = entry.dll.decode().lower()
            if not dll.endswith('.dll'):
                #print("warning: {}".format(dll))
                dll = "{}.dll".format(dll.split('.dll')[0])
            dlls.append(dll)
            for imp in entry.imports:
                imp = imp.name
                if imp:
                    imp = imp.decode().lower()
                    imp = 'imp_{}'.format(imp)
                    imps.append(imp)

    return dlls, imps

def parse_exports(pe):

    exps = []
    if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
        if len(pe.DIRECTORY_ENTRY_EXPORT.symbols):
            for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                exp = exp.name
                if exp:
                    exp = exp.decode().lower()
                    exp = 'exp_{}'.format(exp)
                    exps.append(exp)

    return exps

def parse_sections(pe):
    secs = {}
    num = 1
    entrypoint_addr = pe.OPTIONAL_HEADER.AddressOfEntryPoint
    entrypoint_valid = False
    for section in pe.sections:
        features = {}
        try:
            features['name'] = section.Name.decode().rstrip('\0')
        except:
            features['name'] = str(section.Name)

        # DON"T PANIC! just tryting to extract each bit of charactersitics into a separate feature. 32 is the length of Characteristics field
        characteristics = section.Characteristics
        characteristics = bin(characteristics)[2:]
        characteristics = '0' * (32 - len(characteristics)) + characteristics
        for i in range(32):
            features['characteristics_bit{}'.format(i)] = (characteristics[31-i] == '1')


        features['size']                            = section.SizeOfRawData
        #features['virtualSize']                     = section.Misc_VirtualSize
        #features['virtualAddress']                  = section.VirtualAddress
        #features['physicalAddress']                 = section.Misc_PhysicalAddress
        features['entropy']                         = section.get_entropy()
        #features['rawAddress(pointerToRawData)']    = section.PointerToRawData
        #features['pointerToRelocations']            = section.PointerToRelocations
        #features['numberOfRelocations']             = section.NumberOfRelocations
        virtualAddress = section.VirtualAddress
        virtualSize = section.Misc_VirtualSize
        for fname, fvalue in features.items():
            secs['pesection_{}_{}'.format(num, fname)] = fvalue

        if entrypoint_addr >= virtualAddress and (entrypoint_addr - virtualAddress) < virtualSize: # this is the sections which entry point is in it!!!
            for fname, fvalue in features.items():
                secs['pesectionProcessed_entrypointSection_{}'.format(fname)] = fvalue
            entrypoint_valid = True

        num += 1

    if not entrypoint_valid:
        return

    entropies = [value for feature, value in secs.items() if feature.endswith('_entropy')]
    if len(entropies):
        mean_entropy = sum(entropies) / float(len(entropies))
        min_entropy = min(entropies)
        max_entropy = max(entropies)
    else:
        mean_entropy = 0
        min_entropy = 0
        max_entropy = 0

    sizes = [value for feature, value in secs.items() if feature.endswith('_size')]
    if len(sizes):
        mean_size = sum(sizes) / float(len(sizes))
        min_size = min(sizes)
        max_size = max(sizes)
    else:
        mean_size = 0
        min_size = 0
        max_size = 0

    virtual_sizes = [value for feature, value in secs.items() if feature.endswith('_virtualSize')]
    if len(virtual_sizes):
        mean_virtual_size = sum(virtual_sizes) / float(len(virtual_sizes))
        min_virtual_size = min(virtual_sizes)
        max_virtual_size = max(virtual_sizes)
    else:
        mean_virtual_size = 0
        min_virtual_size = 0
        max_virtual_size = 0

    secs['pesectionProcessed_sectionsMeanEntropy']      = mean_entropy
    secs['pesectionProcessed_sectionsMinEntropy']       = min_entropy
    secs['pesectionProcessed_sectionsMaxEntropy']       = max_entropy

    secs['pesectionProcessed_sectionsMeanSize']         = mean_size
    secs['pesectionProcessed_sectionsMinSize']          = min_size
    secs['pesectionProcessed_sectionsMaxSize']          = max_size

    #secs['pesectionProcessed_sectionsMeanVirtualSize']  = mean_virtual_size
    #secs['pesectionProcessed_sectionsMinVirtualSize']   = min_virtual_size
    #secs['pesectionProcessed_sectionsMaxVirtualSize']   = max_virtual_size

    secs.update(parse_resources(pe))

    return secs, num - 1

def parse_pe_header(pe):
    headers = {}
    opt_header = pe.OPTIONAL_HEADER
    fields = ['SizeOfHeaders', 'AddressOfEntryPoint', 'ImageBase', 'SizeOfImage', 'SizeOfCode', 'SizeOfInitializedData', 'SizeOfUninitializedData', 'BaseOfCode', 'BaseOfData', 'SectionAlignment', 'FileAlignment']
    for f in fields:
        headers['header_{}'.format(f)] = getattr(opt_header, f)

    coff_header = pe.FILE_HEADER
    fields = ['NumberOfSections', 'SizeOfOptionalHeader']
    for f in fields:
        headers['header_{}'.format(f)] = getattr(coff_header, f)

    # DON"T PANIC! just trying to extract each bit of charactersitics into a separate feature. 32 is the length of Characteristics field
    characteristics = coff_header.Characteristics
    characteristics = bin(characteristics)[2:]
    characteristics = '0' * (16 - len(characteristics)) + characteristics
    for i in range(16):
        headers['header_characteristics_bit{}'.format(i)] = (characteristics[15-i] == '1')

    return headers

def parse_pe(sample_file):
    try:
        #print("extracting features for sample: {}".format(sample_file))
        pe = pefile.PE(sample_file)
        sha1 = util.compute_sha1(sample_file)
        _, byte_hist = byte_histogram(sample_file)
        byte_hist = dict(enumerate(byte_hist))
        keys_values = byte_hist.items()
        byte_hist = {'freq_byte_' + str(key): value for key, value in keys_values}
        #if pe.FILE_HEADER.Machine == 332: # we only 32bit samples
        dlls, imps = parse_imports(pe)
        exps = parse_exports(pe)
        x = parse_sections(pe)
        if x:
            sections, _ = x
        else:
            return None

        # assert 'pesection_1_name' in sections and sections['pesection_1_name'] == '.MPRESS1'
        headers = parse_pe_header(pe)

        richs = rich.parse_richheader(sample_file)
        rich_names = rich.get_rich_names()
        richs = {name: v for name, v in zip(rich_names, richs)}

        entropy, size = file_entropy(sample_file)
        generics = {'generic_fileSize': size, 'generic_fileEntropy': entropy}
        return {'sha1':sha1, 'headers': headers, 'imps': imps, 'exps': exps, 'dlls': dlls, 'sections': sections, 'rich': richs, 'generics': generics, 'histogram': byte_hist}

    except Exception as e:
        print(e)
        return None



def init_cols(features, size):
    features = {f: v for f, v in features.items() }
    cols = features.keys()
    print("adding {} columns".format(len(cols)))
    data = [[v['defval'] for f, v in features.items()]] 
    data = np.repeat(np.array(data), repeats=size, axis=0) 
    df2 = pd.DataFrame(data=data, columns=cols)
    print("appending to the current dataframe")
    print("done with adding the columns")
    #df2.to_csv('Trial.csv', index=False)
    return df2



def build_row_values(v):
    unpacked_sample_sha1 = v['unpacked_sample_sha1']
    sample_sha1 = v['sample_sha1']
    v = v['features']
    x = benign_df[benign_df.sample_sha1 == unpacked_sample_sha1]
    unpacked_sample_id = x.index[0]
    benign = x.iloc[0]['benign']
    malicious = x.iloc[0]['malicious']

    labels = {'sample_sha1': sample_sha1, 'unpacked_sample_sha1': unpacked_sample_sha1, 'unpacked_sample_id': unpacked_sample_id, 'packed': True, 'benign': benign, 'malicious': malicious, 'packer_name': packername, 'source': source}

    return put_features_in_row(v, labels)


def build_row_values_ember(v):
    unpacked_sample_sha1 = -1
    unpacked_sample_id = -1
    sample_sha1 = v['sample_sha1']
    malicious = v['label']
    benign = not v['label']
    v = v['features']
    source = 'wild-ember'
    packername = 'none'

    # print("WARNING:")
    # print("packed value set to the False as default value, you need to update it")
    # print("-----------------")
    packed = False

    labels = {'sample_sha1': sample_sha1, 'unpacked_sample_sha1': unpacked_sample_sha1, 'unpacked_sample_id': unpacked_sample_id, 'packed': packed, 'benign': benign, 'malicious': malicious, 'packer_name': packername, 'source': source}

    return put_features_in_row(v, labels)



def build_wild_pickle_file(res):
    features = get_all_features(res)
    size = len(res)
    df = init_cols(features, size)
    selected_cols = list(df.columns)
    print("The created df has {} columns".format(df.shape))

    c = len(res)
    print("importing {} samples features into the pickle file".format(c))
    for sample_id, v in res.items():
        c -= 1
        sample_id = int(sample_id)
        dlls = []
        for dll in v['dlls']:
            if '.dll' not in dll:
                dll = '{}.dll'.format(dll)
            dlls.append(dll)
    
        imps = v['imps']
        exps = v['exps']
        sections = v['sections']
        headers = v['headers']
        generics = v['generics']
        histogram = v['histogram']
        rich = v['rich']
        sha1 = v['sha1']
        cols = ['sha1']
        vals = [sha1]
        cols += list(headers.keys())
        vals += list(headers.values())
        cols += dlls + imps + exps
        vals += [True] * (len(dlls) + len(imps) + len(exps))
        cols += list(sections.keys())
        vals += list(sections.values())
        cols += list(rich.keys())
        vals += list(rich.values())
        cols += list(generics.keys())
        vals += list(generics.values())
        cols += list(histogram.keys())
        vals += list(histogram.values())
        nb = ['api_import_nb', 'api_export_nb', 'dll_import_nb']
        cols += nb
        vals += [len(imps), len(exps), len(dlls)]
        selected_cols.extend(nb)
        #both = set(df.columns).intersection(cols)
        original_cols = set(df.columns)
        both  = [value for value in cols if value in original_cols] 
        idx = [cols.index(x) for x in both]
        cols = [cols[i] for i in idx]
        vals = [vals[i] for i in idx]
        df.loc[sample_id, cols] = vals

        if c % 1000 == 0:
            print(c)
      
    return df


def get_all_features(res):
    IMPEXP_THR = 0.002
    all_features = {}
    cnt = Counter()
    for _, features in res.items():
        try:
            if 'features' in features:
                features = features['features']
        except TypeError as e:
            continue
        all_features['sha1'] = {'dtype': int, 'defval': 0}

        for f, _ in features['headers'].items():
            all_features[f] = {'dtype': int, 'defval': 0}

        for f in features['imps'] + features['exps']:
            cnt[f] += 1
            all_features[f] = {'dtype': bool, 'defval': False}
        for f in features['dlls']:
            if not f.endswith('.dll'):
                f = '{}.dll'.format(f.split('.dll')[0])
            all_features[f] = {'dtype': bool, 'defval': False}

        for f, _ in features['sections'].items():
            if 'name' in f:
                all_features[f] = {'dtype': object, 'defval': 'none'}
            elif 'characteristics' in f:
                all_features[f] = {'dtype': bool, 'defval': 0}
            else:
                all_features[f] = {'dtype': int, 'defval': 0}

        for f in features['rich']:
            all_features[f] = {'dtype': int, 'defval': 0}

        for f in features['generics']:
            all_features[f] = {'dtype': int, 'defval': 0}

        for f in features['histogram']:
            all_features[f] = {'dtype': int, 'defval': 0}

    for f in ['api_import_nb', 'api_export_nb', 'dll_import_nb']:
        all_features[f] = {'dtype': int, 'defval': 0}

    x = len(all_features)
    res = {}
    for f, v in all_features.items():
        if f.startswith('imp_') or f.startswith('exp_'):
            if cnt[f] >= IMPEXP_THR * len(res):
                res[f] = v
        else:
            res[f] = v
    all_features = res

    print("Out of {} features for the new samples, after removing the rare imp/exp features, we have {} features left".format(x, len(all_features)))
    return all_features


def extract_samples_features(rootdir_list, target_file):
    res = {}
    i = 0
    for rootdir in rootdir_list:
        print("Extracting samples features from the directory {}".format(rootdir))
        for subdir, dirs, files in os.walk(rootdir):
            print("Subdirectory {}".format(subdir))
            for file in files:
                sample_path = os.path.join(subdir, file)
                res_i = parse_pe(sample_path)
                if res_i ==None:
                    continue
                res[i] = res_i
                i = i + 1
    df = build_wild_pickle_file(res)
    df.to_csv(target_file, index=False, columns=df.columns)  
    
def find_file(rootdir_list, name):
    for rootdir in rootdir_list:
        for root, dirs, files in os.walk(rootdir):
            if name in files:
                return os.path.join(root, name)

def create_directory_df(rootdir_list):
    cols = ['root', 'file', 'sha1']
    data = []
    for rootdir in rootdir_list:
        print("Extracting path from the directory {}".format(rootdir))
        for subdir, dirs, files in os.walk(rootdir):
            print("Subdirectory {}".format(subdir))
            for file in files:
                sha1 = file.partition('.')
                #Avoiding log files
                if len(sha1[2])>3:
                    pass
                else:
                    sha1 = sha1[0]
                    data.append([subdir,file, sha1])
    newdf = pd.DataFrame(data, columns = cols)
    newdf.to_csv('file_path.csv', index=False)

def add_byte_hist(sha_df, filepath_df):
    c = 0
    byte_hist =[]
    col_names = ["sha1"]
    for i in range (0,256):
        col_names.append('freq_byte_' + str(i))
    df = pd.DataFrame( columns = col_names)
    df.to_csv('hist.csv', index=False)
    for sample in sha_df['sample_sha1']:
        try:
            idx = filepath_df[filepath_df['sha1']==sample].index
        except Exception as e:
            idx = None
        if idx != None:
            path = os.path.join(filepath_df.loc[idx,'root'].values[0], filepath_df.loc[idx,'file'].values[0])
            _, byte_hist_i = byte_histogram(path)
        else:
            byte_hist_i = [None]*256
        byte_hist_i.insert(0,sample)
        byte_hist.append(byte_hist_i)
        c+=1
        if c % 500 == 0 or c == sha_df.shape[0]:
            df = pd.read_csv('hist.csv')
            df_new = pd.DataFrame(byte_hist, columns=df.columns)
            df = pd.concat([df,df_new])
            df.to_csv('hist.csv', index=False)
            byte_hist = []
            print(c)


def process():
    df_balanced = pd.read_pickle("./wild_balanced.pkl")
    print('a')
    hist = pd.read_csv("hist.csv")
    print('b')
    new = hist.iloc[:,1:]
    new = new.reset_index(drop=True)
    df_balanced.drop(columns = ['sample_sha1', 'benign', 'packed', 'unpacked_sample_sha1', 
                                'unpacked_sample_id','packer_name', 'benign_vt', 'malicious_vt', 'similarity',
                                'most_similar_sha1', 'source', 'unpacked_similarity'], inplace=True)
    df_balanced = df_balanced.reset_index(drop=True)
    df_balanced = pd.concat([df_balanced, new  ], axis=1, copy=False )
    df_balanced.to_pickle("./wild_balanced_complete_preprocessed.pkl")

def readfile(filename):
	'''
		Convert file into bytes
	'''
    
	with open(filename, "rb") as b:
		b_bytes = b.read()
	return b_bytes


def winPath2ubuPath(winpath):
    # d,p = os.path.splitdrive(winpath) # NG only works on windows!
    d,p = winpath.split(':')
    ubupath = '/mnt/'+d.lower()+p.replace('\\','/')   
    print (ubupath)
    return ubupath

def extract_single_sample_feature (sample_path):

    with open('data/encoder.pickle', 'rb') as handle:
    #with open('encoder.pickle', 'rb') as handle:

        encoder = pickle.load(handle)
    keys = encoder.keys()

    feature_values = []
    ordered_features = pd.read_csv('data\columns.csv')
    ordered_features = ordered_features['Column_names'][1:]
    res = parse_pe(sample_path)
    if res == None:
        return res
    headers = res['headers']
    imps = res['imps']
    exps = res['exps']
    dlls = res['dlls']
    sections = res['sections']
    rich = res['rich']
    generics = res['generics']
    histogram = res['histogram']
    #Features are mixed therefore we need to order them to match the model
    for feature in ordered_features.values:
        if feature.startswith('header'):
            feature_values.append(headers[feature])
        elif feature.startswith('imp'):
           feature_values.append(int(feature in imps))
        elif feature.startswith('exp'):
           feature_values.append(int(feature in exps)) 
        elif feature.endswith('.dll'):
           feature_values.append(int(feature  in dlls))
        elif feature.startswith('pesection') and feature.endswith('name') :
            try: 
                val = sections[feature]
                if val in keys:
                    val = encoder[val]
                else:
                    val = encoder['Unknown'] 
                feature_values.append(val)
            except:
                feature_values.append(encoder['none'])
        elif (feature.startswith('pesection')) and ('characteristics_bit' in feature):
            try:
                feature_values.append(int(sections[feature]))
            except:
                feature_values.append(0)

        elif feature.startswith('pesection'):
            try: 
                feature_values.append(sections[feature])
            except:
                feature_values.append(0)
        elif feature.startswith('rich'):
                feature_values.append(rich[feature])
        elif feature.startswith('generic'):
                feature_values.append(generics[feature])
        elif feature.startswith('freq'):
                feature_values.append(histogram[feature])
        elif feature == 'api_import_nb': 
            feature_values.append(int(len(imps)))
        elif feature == 'api_exort_nb':
            feature_values.append(int(len(exps)))
        elif feature ==  'dll_import_nb':
            feature_values.append(int(len(dlls)))
        else:
            print(feature)
    return feature_values

if __name__ == '__main__':
    # A list of rootdir because our samples are spread in wild and ember dataset. 
    # It is important to have all the features in one dictionary in order to remove i.e non frequent imports
    rootdir_list = ['D:\\wild\\samples','D:\\wild-ember\\samples',]
    target_file = 'features_wild.csv'
    extract_samples_features(rootdir_list, target_file)



